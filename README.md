# Cold Email Gen (RAG + LangChain + Ollama + Streamlit)

Run locally (no paid API):
1. Install Ollama: https://ollama.com (pull a model, e.g., `ollama pull llama3`) and run the Ollama server.
2. Create virtualenv and install requirements: `pip install -r requirements.txt`
3. Copy `.env.example` to `.env` and adjust if needed.
4. Run: `streamlit run streamlit_app.py`